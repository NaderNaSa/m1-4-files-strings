{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Workshop Python Intro.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPMuPsz+efoYpJzg8ElS0Ut"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"colab":{},"colab_type":"code","id":"Bg811t1qthd4"},"source":["# Warmup: Counter. \n","\n","Count how many times each element in a list occurs.\n","\n","```\n","[1, 3, 2, 1, 5, 3, 5, 1, 4] â‡’\n","\n","    1: 3 times\n","    2: 1 time\n","    3: 2 times\n","    4: 1 time\n","    5: 2 times\n","```\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["1 :  3 times\n2 :  1 time\n3 :  2 times\n4 :  1 time\n5 :  2 times\n"]}],"source":["from collections import Counter\n","# declaring the list \n","l = [1, 3, 2, 1, 5, 3, 5, 1, 4]\n","# driver program \n","d = Counter(l) \n","for i in set(l):\n","    if (d[i] > 1) :\n","        print('{} :  {} times'.format(i, d[i])) \n","    else:\n","        print('{} :  {} time'.format(i, d[i])) "]},{"cell_type":"markdown","metadata":{},"source":["# Safe dict reading\n","\n","define a function `safe_dict(d, k)` that takes in a python dict `d` and a key `k` and makes it safe to read even with keys that aren't in the dictionary. If you try to read from the dictionary with a bad key, it should return 0 instead.\n","\n","```\n","d = {1 : 2, 3 : 4}\n","safe_dict(d, 1) -> 2\n","safe_dict(d, 'cat') -> 0\n","```"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["def safe_dict(d,k):\n","    try:\n","        result = d[k]\n","    except KeyError as ke:\n","        result = 0\n","    return result"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["2\n0\n"]}],"source":["print(safe_dict({1:2,3:4},1))\n","print(safe_dict({1:2,3:4},'cat'))"]},{"cell_type":"markdown","metadata":{"colab":{},"colab_type":"code","id":"tl_ZhkbEtiTD"},"source":["# File Reading: Hamlet Exercises\n","\n","Open `hamlet.txt` in the `data` folder\n","\n","### 1. Mentionned Hamlet\n","\n","How many times is hamlet mentioned in the book?\n","\n","Use python and line iteration to count it up"]},{"cell_type":"code","execution_count":35,"metadata":{"tags":[]},"outputs":[],"source":["'''\n","open the file\n","read contecnt of the file to string\n","then get number of accurences \n","'''\n","def count_hamlet() :\n","    occurrances=0\n","    f = open(\"data\\hamlet.txt\",\"r\")\n","    data = f.read().upper()\n","    occurrances = data.count(\"HAMLET\")\n","    print(f\"Number of occurrences of the word HAMLET is: {occurrances}\")\n","    #f.close\n","    return occurrances"]},{"cell_type":"code","execution_count":17,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of occurrences of the word HAMLET is: 474\n474\n"]}],"source":["print(count_hamlet())"]},{"cell_type":"markdown","metadata":{},"source":["### 2. File Reading as a .py program\n","\n","Make a python file that defines a function that counts the number of times hamlet is mentionned using the code in the previous exercise.\n","\n","Then import it in your notebook and call it here."]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of occurrences of the word HAMLET is: 474\n"]},{"output_type":"execute_result","data":{"text/plain":["474"]},"metadata":{},"execution_count":18}],"source":["\n","import find_hamlet\n","count_hamlet()"]},{"cell_type":"markdown","metadata":{},"source":["### 3. Unique words in hamlet\n","\n","Write a program that counts the unique words in hamlet."]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["def count_unique_words() :\n","    file =  open(\"data\\hamlet.txt\",\"r\")\n","    lines = file.read().splitlines()\n","    uniques = set()\n","    for line in lines:\n","        nline = line.replace(\".\",\" \")\n","        nline = nline.replace(\",\",\" \")\n","        nline = nline.replace('\"',\" \")\n","        nline = nline.replace(';',\" \")\n","        nline = nline.replace('[',\" \")\n","        nline = nline.replace(']',\" \")\n","        nline = nline.replace('_',\" \")\n","        nline = nline.replace('(',\" \")\n","        nline = nline.replace(')',\" \")\n","        nline = nline.replace('!',\" \")\n","        nline = nline.replace('?',\" \")\n","        nline = nline.replace('-',\" \")\n","        nline = nline.replace(':',\" \")\n","        nline = nline.replace('/',\" \")\n","        nline = nline.replace('\\\\',\" \")\n","        nline = nline.replace('@',\" \")\n","        nline = nline.replace('#',\" \")\n","        nline = nline.replace('^',\" \")\n","        nline = nline.replace('&',\" \")\n","\n","     \n","        uniques |= set(nline.split())\n","        \n","\n","    return len(uniques) #print(f\"Unique words: {len(uniques)}\")"]},{"cell_type":"code","execution_count":38,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["5458\n"]}],"source":["print(count_unique_words())"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# File Reading 2: A Python library.\n","\n","In the `data` folder, you will find a folder called `csrgraph` which is a python library.\n","\n","### 1. File count\n","\n","Count the `py` files in the library using the `os` package"]},{"cell_type":"code","execution_count":137,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["8\n"]}],"source":["import os\n","count=0\n","for file in os.listdir(\"..\\data\\csrgraph\"):\n","    if file.endswith(\".py\"):\n","        count += 1\n","print(count)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### 2. For the following packages, count the number of files that import them:\n","\n","- pandas \n","\n","- numpy\n","\n","- numba"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#that import all of them or one of them"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["pandas :   4\nnumpy :   6\nnumba :   6\nsklearn :   4\nNone\n"]}],"source":["'''\n","loop packages\n","loop files for each package\n","search for the packed in the file\n","\n","'''\n","def countimportfiles(packages):\n","    path= \"..\\data\\csrgraph\"\n","    fileslist = os.listdir(path)\n","    count= 0\n","    for p in packages:\n","        count = 0\n","        for f in fileslist:\n","            if f.endswith(\".py\"):\n","                file =  open(path + '/' + f,\"r\")\n","                openfile = file.read()\n","                if  openfile.find('import '+p) != -1 or openfile.find('from '+p) != -1:\n","                    count += 1\n","                \n","        print (f\"{p} :   {str(count)}\" )\n","\n","print(countimportfiles([\"pandas\",\"numpy\",\"numba\",\"sklearn\"]))    "]},{"cell_type":"markdown","metadata":{},"source":["# First NLP Program: IDF\n","\n","Given a list of words, the the inverse document frequency (IDF) is a basic statistic of the amount of information of each word in the text.\n","\n","The IDF formulat is:\n","\n","$$IDF(w) = ln(\\dfrac{N}{1 + n(w)})$$\n","\n","Where:\n","\n","- $w$ is the token (unique word),\n","- $n(w)$ is the number of documents that $w$ occurs in,\n","- $N$ is the total number of documents\n","\n","Write a function, `idf(docs)` that takes in a list of lists of words and returns a dictionary  `word -> idf score`\n","\n","Example:\n","\n","```\n","IDF([['interview', 'questions'], ['interview', 'answers']]) -> {'questions': 0.0, \n","                                                                'interview': -0.4, \n","                                                                'answers': 0.0}\n","\n","\n","```"]},{"cell_type":"code","execution_count":164,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["answers : 0.0\ninterview : -0.40546510810816444\nquestions : 0.0\n"]}],"source":["import numpy as np\n","\n","def IDF(docs):\n","    #get the lsit of unqiue word\n","    u_words = np.unique(docs)\n","    N = len(docs)\n","    ln=len(u_words)\n","    #loop unique words\n","    word_document_frequency = []\n","    for uw in u_words:\n","\n","        #loop documents\n","        uw_count=0\n","        for d in docs:\n","            if bool(np.intersect1d(uw,d)) == True:\n","                uw_count += 1\n","            \n","        word_document_frequency.append((uw,uw_count))\n","    \n","    for w in range(len(word_document_frequency)):\n","        result = np.log(N/(1 + (word_document_frequency[w][1])))\n","        print(f\"{word_document_frequency[w][0]} : {result}\")\n","  \n","IDF([['interview', 'questions'], ['interview', 'answers']])"]},{"cell_type":"markdown","metadata":{"colab":{},"colab_type":"code","id":"82bfnc_KueoX"},"source":["# Stretch Goal: IDF on Hamlet\n","\n","Calculate the IDF dictionary on the Hamlet book.\n","\n","What's the IDF of \"Hamlet\"?\n","\n","What's the word with the highest IDF in the book?"]},{"source":["\n","The IDF formulat is:\n","\n","$$IDF(w) = ln(\\dfrac{N}{1 + n(w)})$$\n","\n","Where:\n","\n","- $w$ is the token (unique word),\n","- $n(w)$ is the number of documents that $w$ occurs in,\n","- $N$ is the total number of documents\n"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["6680\n5458\n"]}],"source":["#\n","# \n","#            not compleeted\n","# \n","# \n","num_lines = sum(1 for line in open('data\\hamlet.txt'))\n","unique_words= count_unique_words()\n","print(num_lines)\n","print(unique_words)"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["the IDF of Hamlet is: -6.163314804034641\n"]}],"source":["import numpy as np\n","import find_hamlet as fh\n","uw = count_unique_words()\n","\n","cnt = fh.count_hamlet()\n","result = np.log(1/(1 + (1 * cnt)))\n","print(f\"the IDF of Hamlet is: {result}\")"]},{"cell_type":"code","execution_count":24,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["cvec_count \nSparse Matrix Shape :  (0, 13644)\nNon Zero Count :  0\n                   term  occurrences\n0                    _a            0\n9088            praying            0\n9090           prayÃ¢ st            0\n9091          preaching            0\n9092   preaching stones            0\n9093          precedent            0\n9094     precedent lord            0\n9095    precedent peace            0\n9096          preceding            0\n9097    preceding fates            0\n9098           precepts            0\n9099      precepts gave            0\n9100       precepts thy            0\n9101           precious            0\n9102    precious diadem            0\n9103  precious instance            0\n9104          precisely            0\n9105      precisely thÃ¢            0\n9106           precurse            0\n9107    precurse fierce            0\n"]},{"output_type":"error","ename":"ValueError","evalue":"Found array with 0 sample(s) (shape=(0, 13644)) while a minimum of 1 is required.","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32m<ipython-input-24-6abbbefb6719>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm_freq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mtransformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mtransformed_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcvec_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformed_weights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mweight_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'term'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mcvec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'weight'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    688\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 690\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    691\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1430\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mmatrix\u001b[0m \u001b[0mof\u001b[0m \u001b[0mterm\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0mcounts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1431\u001b[0m         \"\"\"\n\u001b[1;32m-> 1432\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'csc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1433\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1434\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    648\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 650\u001b[1;33m             raise ValueError(\"Found array with %d sample(s) (shape=%s) while a\"\n\u001b[0m\u001b[0;32m    651\u001b[0m                              \u001b[1;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m                              % (n_samples, array.shape, ensure_min_samples,\n","\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 13644)) while a minimum of 1 is required."]}],"source":["import pandas as pd\n","import numpy as np\n","from itertools import islice\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","data = open('data\\hamlet.txt', 'r+')\n","#print(data.read())\n","cvec = CountVectorizer(stop_words='english', min_df=1, max_df=.5, ngram_range=(1,2))\n","cvec.fit(data)\n","list(islice(cvec.vocabulary_.items(), 20))\n","len(cvec.vocabulary_)\n","cvec_count = cvec.transform(data)\n","print(f'cvec_count {cvec_count}' )\n","print('Sparse Matrix Shape : ', cvec_count.shape)\n","print('Non Zero Count : ', cvec_count.nnz)\n","#print('sparsity: %.2f%%' % (100 * cvec_count.nnz / (cvec_count.shape[0] * cvec_count.shape[1])))\n","\n","occ = np.asarray(cvec_count.sum(axis=0)).ravel().tolist()\n","count_df = pd.DataFrame({'term': cvec.get_feature_names(), 'occurrences' : occ})\n","term_freq = count_df.sort_values(by='occurrences', ascending=False).head(20)\n","print(term_freq)\n","transformer = TfidfTransformer()\n","transformed_weights = transformer.fit_transform(cvec_count)\n","weights = np.asarray(transformed_weights.mean(axis=0)).ravel().tolist()\n","weight_df = pd.DataFrame({'term' : cvec.get_feature_names(), 'weight' : weights})\n","tf_idf = weight_df.sort_values(by='weight', ascending=False).head(20)\n","print(tf_idf) "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["file = open(\"data\\hamlet.txt\",'r')\n","data = file.read()\n","data = [\n","    [(word.replace(\",\", \"\")\n","          .replace(\".\", \"\")\n","          .replace(\"(\", \"\")\n","          .replace(\")\", \"\"))\n","    for word in row.lower().split(\" \")]\n","    for row in data]\n","\n","#Removes header\n","#data# = data[1:]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}]}